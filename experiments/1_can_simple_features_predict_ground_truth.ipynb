{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "id": "d87f5c27dafdc9e2",
    "outputId": "67fdd7df-e3cf-40ae-8230-15d52afc9b77",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d87f5c27dafdc9e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "outputId": "391344dc-3d74-4b3e-9ba1-9ba9d9a8d24a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.classification_utils import _evaluate_predictive_method_from_arrays, evaluate_predictive_method\n",
    "from src.readability_diversity_metrics import compute_diversity_and_readability_metrics\n",
    "from src.results_loaders import CLadder, ProntoQA, ngram_vectorize_new, select_features, HelmResultsLoader, \\\n",
    "    EvalsResultsLoader\n",
    "from src.utils import load_with_conditions, save_dataframe, Cohen_correction, initialize_instance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Fit classifier using simple features to predict the ground truth  ",
   "metadata": {
    "collapsed": false,
    "id": "d5924522ca1da765"
   },
   "id": "d5924522ca1da765"
  },
  {
   "metadata": {
    "id": "c3a1900ea00592a4"
   },
   "cell_type": "markdown",
   "source": "List the various datasets from HELM and KindsOfReasoning and the datasets with specialised loaders.",
   "id": "c3a1900ea00592a4"
  },
  {
   "metadata": {
    "id": "66bb0c83098b9f68"
   },
   "cell_type": "code",
   "source": [
    "helm_list = [('legalbench', 'abercrombie'), ('legalbench', 'corporate_lobbying'), ('legalbench', 'function_of_decision_section'), ('legalbench', 'proa'), ('legalbench', 'international_citizenship_questions')]"
   ],
   "id": "66bb0c83098b9f68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5b41f51eb6cfc5ac"
   },
   "cell_type": "code",
   "source": [
    "evals_list = ['fantasy_reasoning',\n",
    " 'neubaroco',\n",
    " 'moral_permissibility',\n",
    " 'causal_judgment',\n",
    " 'metaphor_boolean',\n",
    " 'commonsense_qa_2',\n",
    " 'space_nli',\n",
    " 'anli',\n",
    " 'wanli',\n",
    " 'babi_task_16',\n",
    " 'formal_fallacies_syllogisms_negation']"
   ],
   "id": "5b41f51eb6cfc5ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "702d4081da50693b"
   },
   "cell_type": "code",
   "source": "total_list = [(EvalsResultsLoader, eval) for eval in evals_list] + [(HelmResultsLoader, scenario, subscenario) for scenario, subscenario in helm_list] + [(CLadder,), (ProntoQA,)]",
   "id": "702d4081da50693b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6eacd8d335045aa4"
   },
   "cell_type": "markdown",
   "source": [
    "### Now train classifiers using the n-grams."
   ],
   "id": "6eacd8d335045aa4"
  },
  {
   "metadata": {
    "id": "e34975266166586b"
   },
   "cell_type": "code",
   "source": [
    "def _check_skip(res_df, pred_method_name, feature_name, dataset_name):\n",
    "    if len(res_df) > 0 and len(res_df[(res_df[\"predictive_method\"] == pred_method_name) & (\n",
    "            res_df[\"features\"] == feature_name) & (res_df[\"dataset\"] == dataset_name)]) > 0:\n",
    "        print(f\"Skipping {feature_name}, {pred_method_name} because it is already in the dataframe\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Doing {feature_name}, {pred_method_name}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def _concat_and_save(res_df, pred_method_name, feature_name, dataset_name, group,\n",
    "                     Accuracy_val, Accuracy_test, instance_level_predictions_val, instance_level_predictions_test,\n",
    "                     trained_method, filename):\n",
    "    res_df = pd.concat([res_df, pd.DataFrame(\n",
    "        [{\"predictive_method\": pred_method_name, \"features\": feature_name, \"trained_classifier\": trained_method,\n",
    "          \"Accuracy_val\": Accuracy_val, \"Accuracy_test\": Accuracy_test,\n",
    "          \"instance_level_predictions_val\": np.array(instance_level_predictions_val),\n",
    "          \"instance_level_predictions_test\": np.array(instance_level_predictions_test),\n",
    "          \"dataset\": dataset_name, \"group\": group}], index=[0])])\n",
    "\n",
    "    save_dataframe(filename, res_df)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def evaluate_and_update(res_df, train_df, validation_df, test_df, features, predictive_method,\n",
    "                        pred_method_name, feature_name, dataset_name, group,\n",
    "                        filename, **kwargs):\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name):\n",
    "        results = evaluate_predictive_method(train_df, test_df,\n",
    "                                             features,\n",
    "                                             f\"ideal_encoded\",\n",
    "                                             predictive_method=predictive_method,\n",
    "                                             return_instance_level_predictions=True,\n",
    "                                             return_trained_method=True, binary=False,\n",
    "                                             compute_accuracy=True,\n",
    "                                             **kwargs)\n",
    "        Accuracy_test, trained_method, instance_level_predictions_test = results[\"accuracy\"], results[\n",
    "            \"method_instance\"], \\\n",
    "            results[\"instance_level_predictions\"]\n",
    "\n",
    "        results = evaluate_predictive_method(train_df, validation_df,\n",
    "                                             features,\n",
    "                                             f\"ideal_encoded\",\n",
    "                                             predictive_method=predictive_method,\n",
    "                                             return_instance_level_predictions=True,\n",
    "                                             trained_method=trained_method, binary=False, compute_accuracy=True,\n",
    "                                             **kwargs)\n",
    "        Accuracy_val, instance_level_predictions_val = results[\"accuracy\"], results[\"instance_level_predictions\"]\n",
    "\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, dataset_name, group,\n",
    "                                  Accuracy_val, Accuracy_test, instance_level_predictions_val,\n",
    "                                  instance_level_predictions_test,\n",
    "                                  trained_method, filename)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def evaluate_and_update_arrays(res_df, X_train, train_labels, X_val, val_labels,\n",
    "                               X_test, test_labels, predictive_method,\n",
    "                               pred_method_name, feature_name, dataset_name, group,\n",
    "                               filename, **kwargs):\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, dataset_name):\n",
    "        results = _evaluate_predictive_method_from_arrays(X_train,\n",
    "                                                          train_labels,\n",
    "                                                          X_test,\n",
    "                                                          test_labels,\n",
    "                                                          predictive_method=predictive_method,\n",
    "                                                          return_trained_method=True,\n",
    "                                                          return_instance_level_predictions=True,\n",
    "                                                          binary=False,\n",
    "                                                          compute_accuracy=True,\n",
    "                                                          **kwargs)\n",
    "\n",
    "        Accuracy_test, trained_method, instance_level_predictions_test = results[\"accuracy\"], results[\n",
    "            \"method_instance\"], \\\n",
    "            results[\"instance_level_predictions\"]\n",
    "\n",
    "        results = _evaluate_predictive_method_from_arrays(X_train,\n",
    "                                                          train_labels,\n",
    "                                                          X_val,\n",
    "                                                          val_labels,\n",
    "                                                          predictive_method=predictive_method,\n",
    "                                                          return_instance_level_predictions=True,\n",
    "                                                          trained_method=trained_method, binary=False,\n",
    "                                                          compute_accuracy=True,\n",
    "                                                          **kwargs)\n",
    "        Accuracy_val, instance_level_predictions_val = results[\"accuracy\"], results[\"instance_level_predictions\"]\n",
    "\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, dataset_name, group,\n",
    "                                  Accuracy_val, Accuracy_test, instance_level_predictions_val,\n",
    "                                  instance_level_predictions_test,\n",
    "                                  trained_method, filename)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def add_baselines(res_df, instance, ideal_col_name, train_df, val_df, test_df, dataset_name, group,\n",
    "                  filename):\n",
    "    # random guess:\n",
    "    pred_method_name = \"random_guess\"\n",
    "    feature_name = \"random_guess\"\n",
    "\n",
    "    # count how many unique values there are in the ground truth\n",
    "    n_choices = len(instance.results_df[ideal_col_name].unique())\n",
    "    random_guess_accuracy = 1 / n_choices\n",
    "    random_guess_instance_level_predictions_val = None\n",
    "    random_guess_instance_level_predictions_test = None\n",
    "\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, dataset_name):\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, dataset_name, group,\n",
    "                                  random_guess_accuracy, random_guess_accuracy,\n",
    "                                  random_guess_instance_level_predictions_val,\n",
    "                                  random_guess_instance_level_predictions_test,\n",
    "                                  None, filename)\n",
    "\n",
    "    # most likely option\n",
    "    pred_method_name = \"most_likely_answer\"\n",
    "    feature_name = \"most_likely_answer\"\n",
    "\n",
    "    # now extract the most likely option in the train set\n",
    "    most_likely_answer = train_df[ideal_col_name].value_counts().idxmax()\n",
    "    most_likely_answer_instance_level_predictions_val = most_likely_answer\n",
    "    most_likely_answer_instance_level_predictions_test = most_likely_answer\n",
    "    most_likely_answer_accuracy_val = (val_df[ideal_col_name] == most_likely_answer).mean()\n",
    "    most_likely_answer_accuracy_test = (test_df[ideal_col_name] == most_likely_answer).mean()\n",
    "\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, dataset_name):\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, dataset_name, group,\n",
    "                                  most_likely_answer_accuracy_val, most_likely_answer_accuracy_test,\n",
    "                                  most_likely_answer_instance_level_predictions_val,\n",
    "                                  most_likely_answer_instance_level_predictions_test,\n",
    "                                  None, filename)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, use_word2vec=False, use_fasttext=False, verbose=True):\n",
    "    instance, dataset_name, ideal_col_name, group = initialize_instance(dataset)\n",
    "\n",
    "    if use_word2vec:\n",
    "        print(\"Extracting word2vec embeddings\")\n",
    "        instance.extract_simple_embeddings(skip_na_rows=False, embedding_model=\"word2vec\", filter_stopwords=True)\n",
    "        print(\"Done\")\n",
    "    if use_fasttext:\n",
    "        print(\"Extracting fasttext embeddings\")\n",
    "        instance.extract_simple_embeddings(skip_na_rows=False, embedding_model=\"fasttext\", filter_stopwords=True)\n",
    "        print(\"Done\")\n",
    "\n",
    "    train_df, val_df, test_df = instance.train_val_test_split(discard_na_rows=False, rng=np.random.RandomState(42),\n",
    "                                                              train_size=0.6, val_size=0.2)\n",
    "    if verbose:\n",
    "        print(f'{dataset_name} train_df shape: {train_df.shape}')\n",
    "        print(f'{dataset_name} val_df shape: {val_df.shape}')\n",
    "        print(f'{dataset_name} test_df shape: {test_df.shape}')\n",
    "\n",
    "    primal_train_labels = train_df[ideal_col_name]\n",
    "    primal_val_labels = val_df[ideal_col_name]\n",
    "    primal_test_labels = test_df[ideal_col_name]\n",
    "\n",
    "    # do the encoding of the labels (this is in principle not needed for the ones where the answer is binary)\n",
    "    label_encoder = LabelEncoder()\n",
    "    primal_train_labels_encoded = label_encoder.fit_transform(primal_train_labels)\n",
    "    primal_val_labels_encoded = label_encoder.transform(primal_val_labels)\n",
    "    primal_test_labels_encoded = label_encoder.transform(primal_test_labels)\n",
    "\n",
    "    # add them to the column ideal_encoded\n",
    "    train_df[\"ideal_encoded\"] = primal_train_labels_encoded\n",
    "    val_df[\"ideal_encoded\"] = primal_val_labels_encoded\n",
    "    test_df[\"ideal_encoded\"] = primal_test_labels_encoded\n",
    "\n",
    "    train_prompts = list(train_df[\"prompt\"])\n",
    "    val_prompts = list(val_df[\"prompt\"])\n",
    "    test_prompts = list(test_df[\"prompt\"])\n",
    "\n",
    "    return instance, dataset_name, ideal_col_name, group, train_df, val_df, test_df, train_prompts, val_prompts, test_prompts, primal_train_labels_encoded, primal_val_labels_encoded, primal_test_labels_encoded"
   ],
   "id": "e34975266166586b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make a table with the various datasets and their sizes",
   "id": "46b026f3a2f30cd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_sizes = []\n",
    "dataset_names = []\n",
    "number_of_choices = []\n",
    "\n",
    "for dataset in tqdm(total_list):\n",
    "    instance, dataset_name, ideal_col_name, group, train_df, val_df, test_df, train_prompts, val_prompts, test_prompts, primal_train_labels_encoded, primal_val_labels_encoded, primal_test_labels_encoded = prepare_dataset(dataset, False, False, verbose=False)\n",
    "    dataset_sizes.append(instance.results_df.shape[0])\n",
    "    dataset_names.append(dataset_name)\n",
    "    all_labels = len(set(list(primal_train_labels_encoded) +  list(primal_val_labels_encoded) + list(primal_test_labels_encoded)))\n",
    "    number_of_choices.append(all_labels)\n",
    "    "
   ],
   "id": "2576bf83b977b9db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make a dataframe\n",
    "dataset_sizes_df = pd.DataFrame({\"dataset\": dataset_names, \"size\": dataset_sizes, \"number_of_choices\": number_of_choices})\n",
    "# print to latex\n",
    "print(dataset_sizes_df.to_latex(index=False))"
   ],
   "id": "d1e51763b76160be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now create/load the dataframe with the results. The code loads it if it exists and avoids re-computing the results already available.",
   "id": "1ada5c7d9c3f1a20"
  },
  {
   "metadata": {
    "id": "86887398c993673"
   },
   "cell_type": "code",
   "source": "filename = \"results/ground_truth_prediction_performance.pkl\"",
   "id": "86887398c993673",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "10ddd402b7ef6fc0",
    "outputId": "70255bd8-4974-4c22-b640-2c4a16d2b3ea",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "primal_performance_df = load_with_conditions(filename, overwrite_res)"
   ],
   "id": "10ddd402b7ef6fc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "primal_performance_df.shape"
   ],
   "metadata": {
    "id": "5Ao5rU_Vz44P",
    "outputId": "fb5c27db-c64b-41e7-84fa-60b4852fa047",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "5Ao5rU_Vz44P",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a9e193e7b113aac8"
   },
   "cell_type": "markdown",
   "source": [
    "Now train the various classifiers."
   ],
   "id": "a9e193e7b113aac8"
  },
  {
   "metadata": {
    "id": "6974fbc8f838f530"
   },
   "cell_type": "code",
   "source": [
    "classifiers_list = [(LogisticRegression, {}, \"logistic_regression_l2\"), (LogisticRegression, {\"penalty\": \"l1\", \"solver\": \"liblinear\"}, \"logistic_regression_l1_c=1\"), (LogisticRegression, {\"penalty\": \"l1\", \"solver\": \"liblinear\", \"C\": 0.1}, \"logistic_regression_l1_c=0.1\"), (XGBClassifier, {}, \"xgboost\")]"
   ],
   "id": "6974fbc8f838f530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "bf2252cc000970ad"
   },
   "cell_type": "markdown",
   "source": [
    "First do the baselines and the readability metrics"
   ],
   "id": "bf2252cc000970ad"
  },
  {
   "metadata": {
    "id": "91f849cd293f30d9"
   },
   "cell_type": "code",
   "source": [
    "use_readability_diversity_metrics = True"
   ],
   "id": "91f849cd293f30d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in tqdm(total_list):\n",
    "    instance, dataset_name, ideal_col_name, group, train_df, val_df, test_df, train_prompts, val_prompts, test_prompts, primal_train_labels_encoded, primal_val_labels_encoded, primal_test_labels_encoded = prepare_dataset(dataset)\n",
    "\n",
    "    if use_readability_diversity_metrics:\n",
    "        train_metrics = compute_diversity_and_readability_metrics(train_prompts)\n",
    "        val_metrics = compute_diversity_and_readability_metrics(val_prompts)\n",
    "        test_metrics = compute_diversity_and_readability_metrics(test_prompts)\n",
    "\n",
    "        X_train_metrics = pd.DataFrame(train_metrics).values\n",
    "        X_val_metrics = pd.DataFrame(val_metrics).values\n",
    "        X_test_metrics = pd.DataFrame(test_metrics).values\n",
    "\n",
    "    # baselines:\n",
    "    #  - most likely answer\n",
    "    #  - random guess (= 1/n_choices)\n",
    "\n",
    "    primal_performance_df = add_baselines(primal_performance_df, instance, ideal_col_name, train_df, val_df, test_df, dataset_name, group,\n",
    "                filename)\n",
    "\n",
    "    # now attempt the classification on the primal problem\n",
    "    for predictive_method, kwargs, pred_method_name in classifiers_list:\n",
    "\n",
    "        if use_readability_diversity_metrics:\n",
    "            primal_performance_df = evaluate_and_update_arrays(primal_performance_df, X_train_metrics, primal_train_labels_encoded, X_val_metrics, primal_val_labels_encoded,\n",
    "                                                       X_test_metrics, primal_test_labels_encoded, predictive_method,\n",
    "                                                       pred_method_name, f\"readability_diversity_metrics\", dataset_name, group, filename,\n",
    "                                                       **kwargs)\n",
    ""
   ],
   "metadata": {
    "id": "b2144f9d85dafad",
    "outputId": "fe711982-b656-4cc3-8080-d1a1f25275fe",
    "colab": {
     "referenced_widgets": [
      "86f9f5f1ab3f4385a23b85c59bc871e8"
     ]
    }
   },
   "id": "b2144f9d85dafad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "44ef9416c862e1fb"
   },
   "cell_type": "markdown",
   "source": [
    "Now with the ngrams"
   ],
   "id": "44ef9416c862e1fb"
  },
  {
   "metadata": {
    "id": "ef0546eb5dcdb44e"
   },
   "cell_type": "code",
   "source": [
    "use_ngrams = True\n",
    "use_ngrams_tokenizer = True\n",
    "n_gram_metrics = [\"simple_frequency\", \"tfidf\", \"presence\"]\n",
    "\n",
    "n_gram_sizes = [1, 2]"
   ],
   "id": "ef0546eb5dcdb44e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "da5774b58754d585",
    "outputId": "ca431152-a172-48d3-81c8-d1c9a6ae1985",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9fb8ec8f9e3c4ae7b9a1007f489d9a35",
      "3aa24182a2fe4538932c874e9555d9b4",
      "5f3e5ac6a4fb42c9a524c231abcd92b2",
      "0a96e1241d3f4ee384e904d47d78eed3",
      "706057505f9f4ec0a24c23251dcce780",
      "7d8c0637daff43d4ba018f983e93a10d",
      "295e90c7be1942b79c068b03197d0e1a",
      "76e98745a4dc4bd88b010f95f8218e05",
      "80d2b8bc39e44253a9928c0b655233d0",
      "c2ef91b04d6d4af09cbf02e239526871",
      "fd80d44a1bf74127bee6693fbf0a729b"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in tqdm(total_list):\n",
    "    instance, dataset_name, ideal_col_name, group, train_df, val_df, test_df, train_prompts, val_prompts, test_prompts, primal_train_labels_encoded, primal_val_labels_encoded, primal_test_labels_encoded = prepare_dataset(dataset)\n",
    "\n",
    "    for n_gram_size in n_gram_sizes:\n",
    "        for metric in n_gram_metrics:\n",
    "\n",
    "            if use_ngrams:\n",
    "                #  to do use the new function to extract ngrams as this does not have test data\n",
    "                X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(train_prompts, val_prompts, test_prompts, ngram_range=(1, n_gram_size), metric=metric)\n",
    "                X_train_ngrams_selected, X_val_ngrams_selected, X_test_ngrams_selected, selector = select_features(X_train_ngrams, primal_train_labels_encoded, X_val_ngrams, X_test_ngrams)\n",
    "\n",
    "            if use_ngrams_tokenizer:\n",
    "                #  to do use the new function to extract ngrams as this does not have test data\n",
    "                X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(train_prompts, val_prompts, test_prompts, ngram_range=(1, n_gram_size), use_gpt2_tokeniser=True, metric=metric)\n",
    "                X_train_ngrams_selected_tok, X_val_ngrams_selected_tok, X_test_ngrams_selected_tok, selector_tok = select_features(X_train_ngrams, primal_train_labels_encoded, X_val_ngrams, X_test_ngrams)\n",
    "\n",
    "\n",
    "            # now attempt the classification on the primal problem\n",
    "            for predictive_method, kwargs, pred_method_name in classifiers_list:\n",
    "                # n-grams\n",
    "                if use_ngrams:\n",
    "                    primal_performance_df = evaluate_and_update_arrays(primal_performance_df, X_train_ngrams_selected, primal_train_labels_encoded, X_val_ngrams_selected, primal_val_labels_encoded,\n",
    "                                                                       X_test_ngrams_selected, primal_test_labels_encoded, predictive_method,\n",
    "                                                                       pred_method_name, f\"{n_gram_size}-grams_{metric}\", dataset_name, group, filename,\n",
    "                                                                       **kwargs)\n",
    "\n",
    "                # n-grams with tokenizer\n",
    "                if use_ngrams_tokenizer:\n",
    "                    primal_performance_df = evaluate_and_update_arrays(primal_performance_df, X_train_ngrams_selected_tok, primal_train_labels_encoded, X_val_ngrams_selected_tok, primal_val_labels_encoded,\n",
    "                                                                       X_test_ngrams_selected_tok, primal_test_labels_encoded, predictive_method,\n",
    "                                                                       pred_method_name, f\"{n_gram_size}-grams_{metric}_gpt2\", dataset_name, group, filename,\n",
    "                                                                       **kwargs)\n"
   ],
   "id": "da5774b58754d585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5df597feaae21f0f"
   },
   "cell_type": "markdown",
   "source": "Finally with embeddings (unused in the paper)",
   "id": "5df597feaae21f0f"
  },
  {
   "metadata": {
    "id": "f55e691c7430d507"
   },
   "cell_type": "code",
   "source": [
    "use_word2vec = False\n",
    "use_fasttext = False"
   ],
   "id": "f55e691c7430d507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c11a59b9e0b423bf"
   },
   "cell_type": "code",
   "source": [
    "for dataset in tqdm(total_list):\n",
    "    instance, dataset_name, ideal_col_name, group, train_df, val_df, test_df, train_prompts, val_prompts, test_prompts, primal_train_labels_encoded, primal_val_labels_encoded, primal_test_labels_encoded = prepare_dataset(dataset, use_word2vec, use_fasttext)\n",
    "    # now attempt the classification on the primal problem\n",
    "    for predictive_method, kwargs, pred_method_name in classifiers_list:\n",
    "\n",
    "        # word2vec\n",
    "        if use_word2vec:\n",
    "            primal_performance_df = evaluate_and_update(primal_performance_df, train_df, val_df, test_df,\n",
    "                                                        [\"word2vec_embeddings\"], predictive_method,\n",
    "                                                        pred_method_name, \"word2vec\", dataset_name, group, filename,\n",
    "                                                        **kwargs)\n",
    "\n",
    "        # fasttext\n",
    "        if use_fasttext:\n",
    "            primal_performance_df = evaluate_and_update(primal_performance_df, train_df, val_df, test_df,\n",
    "                                                        [\"fasttext_embeddings\"], predictive_method,\n",
    "                                                        pred_method_name, \"fasttext\", dataset_name, group, filename,\n",
    "                                                        **kwargs)\n"
   ],
   "id": "c11a59b9e0b423bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e5791d1d4fb006c6"
   },
   "cell_type": "code",
   "source": [
    "primal_performance_df.shape"
   ],
   "id": "e5791d1d4fb006c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "15e76131b7a2ff8c",
    "outputId": "021b4640-9160-4d55-f04e-4d4e6c03d2a8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    }
   },
   "cell_type": "code",
   "source": [
    "primal_performance_df.tail()"
   ],
   "id": "15e76131b7a2ff8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract the best classifier and get its accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c9861ab7eb6c1be9"
   },
   "id": "c9861ab7eb6c1be9"
  },
  {
   "metadata": {
    "id": "5838bf5f89429ad0"
   },
   "cell_type": "code",
   "source": "filename = \"results/ground_truth_prediction_performance.pkl\"",
   "id": "5838bf5f89429ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "primal_performance_df = load_with_conditions(filename)\n",
    "print(primal_performance_df.shape)"
   ],
   "metadata": {
    "id": "2e1cd641d8f39898",
    "outputId": "2b7f08c9-bbfc-446d-d364-48d09a38772f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "2e1cd641d8f39898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "primal_performance_df.columns",
   "id": "adb17cfef728cb41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e6530073cccfe545",
    "outputId": "e511f0a8-e8a9-45aa-d06f-ca615a6f289e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    }
   },
   "cell_type": "code",
   "source": [
    "primal_performance_df[\"predictive_method\"].value_counts()"
   ],
   "id": "e6530073cccfe545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "primal_performance_df[\"features\"].value_counts()",
   "id": "bf455de4c081066c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "First consider the best classifier for each feature set."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9fa44debed64fe4b"
   },
   "id": "9fa44debed64fe4b"
  },
  {
   "cell_type": "code",
   "source": [
    "best_predictive_method_per_feature_primal = primal_performance_df.groupby([\"dataset\", \"features\"]).apply(lambda x: x[x.Accuracy_val == x.Accuracy_val.max()]).reset_index(drop=True)\n",
    "# if there are more than one entry with the same accuracy for each [\"dataset\", \"features\"] combination, then pick the first one; also keep the dataset and features columns\n",
    "best_predictive_method_per_feature_primal = best_predictive_method_per_feature_primal.groupby([\"dataset\", \"features\"]).first().reset_index()"
   ],
   "metadata": {
    "id": "4a71431c5d01388f"
   },
   "id": "4a71431c5d01388f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "best_predictive_method_per_feature_primal.shape"
   ],
   "metadata": {
    "id": "2084fa77f0001566",
    "outputId": "bd8db669-3026-4fc8-82ea-4a1014501af2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "2084fa77f0001566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also do the same by excluding the XGBoost classifier\n",
   "id": "1b522b7eb7df4697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# exclude the baselines and xgboost\n",
    "primal_performance_df_no_xgboost = primal_performance_df[~primal_performance_df[\"predictive_method\"].isin([\"xgboost\"])]\n",
    "\n",
    "best_predictive_method_per_feature_primal_no_xgboost = primal_performance_df_no_xgboost.groupby([\"dataset\", \"features\"]).apply(lambda x: x[x.Accuracy_val == x.Accuracy_val.max()]).reset_index(drop=True)\n",
    "# if there are more than one entry with the same accuracy for each [\"dataset\", \"features\"] combination, then pick the first one; also keep the dataset and features columns\n",
    "best_predictive_method_per_feature_primal_no_xgboost = best_predictive_method_per_feature_primal_no_xgboost.groupby([\"dataset\", \"features\"]).first().reset_index()"
   ],
   "id": "5b87c5226edb7790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_predictive_method_per_feature_primal_no_xgboost.shape",
   "id": "4d0a6591f413ef2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "58c37ea50f71adf8"
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Plots with Cohen correction\n",
    "\n",
    "Compute Cohen correction and plot them. The correction is computed by considering the number of choices of each dataset and assuming that a random guess would have an accuracy of 1/n_choices."
   ],
   "id": "58c37ea50f71adf8"
  },
  {
   "metadata": {
    "id": "576b95587a28758c",
    "outputId": "014e6b85-0c6a-4649-fb4d-87acf06dc336",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "best_predictive_method_per_feature_primal.columns"
   ],
   "id": "576b95587a28758c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def manipulate_df(df):\n",
    "    # best_predictive_method_per_feature_primal_pivot = best_predictive_method_per_feature_primal.pivot(index=\"dataset\", columns=\"features\", values=\"Accuracy_test\")\n",
    "    df = df.pivot(index=\"dataset\", columns=\"features\", values=\"Accuracy_test\")\n",
    "    # df.reset_index(inplace=True)\n",
    "    \n",
    "    for column in ['1-grams_presence', '1-grams_presence_gpt2',\n",
    "       '1-grams_simple_frequency', '1-grams_simple_frequency_gpt2',\n",
    "       '1-grams_tfidf', '1-grams_tfidf_gpt2', '2-grams_presence',\n",
    "       '2-grams_presence_gpt2', '2-grams_simple_frequency',\n",
    "       '2-grams_simple_frequency_gpt2', '2-grams_tfidf', '2-grams_tfidf_gpt2',\n",
    "       'readability_diversity_metrics']:\n",
    "        # best_predictive_method_per_feature_primal_pivot[f\"Cohen_{column}\"] = Cohen_correction(best_predictive_method_per_feature_primal_pivot[column], best_predictive_method_per_feature_primal_pivot[\"random_guess\"])\n",
    "        df[f\"Cohen_{column}\"] = Cohen_correction(df[column], df[\"random_guess\"])\n",
    "    \n",
    "    cohen_columns = [col for col in df.columns if \"Cohen\" in col]\n",
    "    df = df[cohen_columns]\n",
    "    # un-pivot\n",
    "    # df = df.melt(id_vars=\"dataset\", value_vars=cohen_columns)\n",
    "    \n",
    "    return df\n",
    "    "
   ],
   "id": "ba7abfe197e9b4bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_predictive_method_per_feature_primal_Cohen = manipulate_df(best_predictive_method_per_feature_primal)\n",
    "best_predictive_method_per_feature_primal_no_xgboost_Cohen = manipulate_df(best_predictive_method_per_feature_primal_no_xgboost)"
   ],
   "id": "54acefc7104d365b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vmin = min(best_predictive_method_per_feature_primal_Cohen.min().min(), best_predictive_method_per_feature_primal_no_xgboost_Cohen.min().min())\n",
    "vmax = max(best_predictive_method_per_feature_primal_Cohen.max().max(), best_predictive_method_per_feature_primal_no_xgboost_Cohen.max().max())\n",
    "print(vmin, vmax)"
   ],
   "id": "53261deb2051b8cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_predictive_method_per_feature_primal_Cohen.columns",
   "id": "46a1f3910b4b4fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets_not_bold = ['CLadder',\n",
    " 'anli',\n",
    " 'causal_judgment',\n",
    " 'commonsense_qa_2',\n",
    " 'formal_fallacies_syllogisms_negation',\n",
    " 'legalbench_abercrombie',\n",
    " 'metaphor_boolean',\n",
    " 'moral_permissibility',\n",
    " 'wanli']\n",
    "\n",
    "\n",
    "\n",
    "def plot_heatmap(df, vmin=None, vmax=None, filename=None, title=None, cbar_label=None):\n",
    "    plot_labels = {\n",
    "        'Cohen_1-grams_presence': \"1-grams Presence, word-level\",\n",
    "        'Cohen_1-grams_presence_gpt2': \"1-grams Presence, token-level\",\n",
    "        'Cohen_1-grams_simple_frequency': \"1-grams TF, word-level\",\n",
    "        'Cohen_1-grams_simple_frequency_gpt2': \"1-grams TF, token-level\",\n",
    "        'Cohen_1-grams_tfidf': \"1-grams TF-IDF, word-level\",\n",
    "        'Cohen_1-grams_tfidf_gpt2': \"1-grams TF-IDF, token-level\",\n",
    "        'Cohen_2-grams_presence': \"2-grams Presence, word-level\",\n",
    "        'Cohen_2-grams_presence_gpt2': \"2-grams Presence, token-level\",\n",
    "        'Cohen_2-grams_simple_frequency': \"2-grams TF, word-level\",\n",
    "        'Cohen_2-grams_simple_frequency_gpt2': \"2-grams TF, token-level\",\n",
    "        'Cohen_2-grams_tfidf': \"2-grams TF-IDF, word-level\",\n",
    "        'Cohen_2-grams_tfidf_gpt2': \"2-grams TF-IDF, token-level\",\n",
    "        'Cohen_readability_diversity_metrics': \"Readability and Diversity Metrics\"\n",
    "    }\n",
    "    \n",
    "    # Rename the columns for the plot\n",
    "    df = df.rename(columns=plot_labels)\n",
    "\n",
    "    # Pivot the DataFrame to get the datasets as rows and features as columns\n",
    "    heatmap_data = df.T\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(20*2/3, 10*2/3))\n",
    "    \n",
    "    # Use a better diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "       \n",
    "    # Create the heatmap with improved aesthetics\n",
    "    sns.heatmap(heatmap_data, \n",
    "                annot=True, \n",
    "                cmap=cmap,\n",
    "                center=0,\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                fmt='.2f',\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={'label': cbar_label, 'shrink': .8},)\n",
    "                # mask=mask)\n",
    "    \n",
    "    plt.xlabel('Datasets', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Rotate and align the tick labels\n",
    "    \n",
    "    datasets = list(df.index)\n",
    "    \n",
    "    ticks = []\n",
    "    bold_indices = []\n",
    "    for dataset in datasets:\n",
    "        if dataset in datasets_not_bold:\n",
    "            ticks.append(f\"{dataset}\")\n",
    "        else:\n",
    "            ticks.append(f\"\\\\textbf{{{dataset}}}\")\n",
    "            bold_indices.append(datasets.index(dataset))\n",
    "    \n",
    "    # plt.xticks(ticks=range(len(ticks)), labels=ticks, rotation=45, ha='right', fontsize=10)   \n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    \n",
    "    # Get the current tick labels\n",
    "    ax = plt.gca()\n",
    "    xtick_labels = ax.get_xticklabels()\n",
    "    \n",
    "    # Bold the specified tick labels\n",
    "    for i in bold_indices:\n",
    "        xtick_labels[i].set_fontweight('bold')\n",
    "\n",
    "    # Adjust the layout to prevent clipping of tick-labels\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    \n",
    "    plt.show()"
   ],
   "id": "deee2dcc51c2c33a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enable LaTeX rendering\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "plot_heatmap(best_predictive_method_per_feature_primal_Cohen, vmin=vmin, vmax=vmax, filename=\"fig/ground_truth_prediction_performance_heatmap.png\", title=\"Normalised accuracy (Cohen's correction)\", cbar_label=\"Cohen's correction\")\n",
    "plot_heatmap(best_predictive_method_per_feature_primal_Cohen, vmin=vmin, vmax=vmax, filename=\"fig/ground_truth_prediction_performance_heatmap.pdf\", title=\"Normalised accuracy (Cohen's correction)\", cbar_label=\"Cohen's correction\")"
   ],
   "id": "83553bbef752d436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_heatmap(best_predictive_method_per_feature_primal_no_xgboost_Cohen, vmin=vmin, vmax=vmax, filename=\"fig/ground_truth_prediction_performance_no_xgboost_heatmap.png\", title=\"Normalised accuracy (Cohen's correction)\", cbar_label=\"Cohen's correction\")\n",
    "plot_heatmap(best_predictive_method_per_feature_primal_no_xgboost_Cohen, vmin=vmin, vmax=vmax, filename=\"fig/ground_truth_prediction_performance_no_xgboost_heatmap.pdf\", title=\"Normalised accuracy (Cohen's correction)\", cbar_label=\"Cohen's correction\")"
   ],
   "id": "e17b8664fd9886c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now make barplots",
   "id": "e9c568fe32a7a994"
  },
  {
   "metadata": {
    "id": "821b5cc4f63927e1",
    "outputId": "5b03be8d-0918-48d8-f0b3-34c783d66070",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_barplot(df):\n",
    "    df = df.reset_index()\n",
    "    df = df.melt(id_vars=\"dataset\", value_vars=df.columns)\n",
    "    # plot the Cohen-corrected values for all features side by side\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.catplot(data=df, x='dataset', y='value',\n",
    "                hue='features', kind='bar', height=5, aspect=2)\n",
    "    plt.ylabel(\"Cohen's correction\")\n",
    "    # rotate x labels\n",
    "    plt.xticks(rotation=90)\n",
    "    # plt.title(f\"{features}\")\n",
    "    plt.show()"
   ],
   "id": "821b5cc4f63927e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_barplot(best_predictive_method_per_feature_primal_Cohen)",
   "id": "ee90456adf6b4c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_barplot(best_predictive_method_per_feature_primal_no_xgboost_Cohen)",
   "id": "b4188006e460bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6976764d0d5bfee4"
   },
   "cell_type": "markdown",
   "source": [
    "For most of these datasets, you have a Cohen's correction that is larger than 0, meaning that there is some predictability with simple features."
   ],
   "id": "6976764d0d5bfee4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9fb8ec8f9e3c4ae7b9a1007f489d9a35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3aa24182a2fe4538932c874e9555d9b4",
       "IPY_MODEL_5f3e5ac6a4fb42c9a524c231abcd92b2",
       "IPY_MODEL_0a96e1241d3f4ee384e904d47d78eed3"
      ],
      "layout": "IPY_MODEL_706057505f9f4ec0a24c23251dcce780"
     }
    },
    "3aa24182a2fe4538932c874e9555d9b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d8c0637daff43d4ba018f983e93a10d",
      "placeholder": "​",
      "style": "IPY_MODEL_295e90c7be1942b79c068b03197d0e1a",
      "value": "100%"
     }
    },
    "5f3e5ac6a4fb42c9a524c231abcd92b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76e98745a4dc4bd88b010f95f8218e05",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_80d2b8bc39e44253a9928c0b655233d0",
      "value": 3
     }
    },
    "0a96e1241d3f4ee384e904d47d78eed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2ef91b04d6d4af09cbf02e239526871",
      "placeholder": "​",
      "style": "IPY_MODEL_fd80d44a1bf74127bee6693fbf0a729b",
      "value": " 3/3 [23:45&lt;00:00, 598.06s/it]"
     }
    },
    "706057505f9f4ec0a24c23251dcce780": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d8c0637daff43d4ba018f983e93a10d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "295e90c7be1942b79c068b03197d0e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76e98745a4dc4bd88b010f95f8218e05": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80d2b8bc39e44253a9928c0b655233d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c2ef91b04d6d4af09cbf02e239526871": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd80d44a1bf74127bee6693fbf0a729b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
